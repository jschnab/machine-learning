{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A neural network for classification of handwritten digits\n",
    "## Introduction\n",
    "In this notebook I present implementation of a neural network, written from scratch, to recognize handwritten digits in the MNIST dataset. The dataset is in a *comma-separated values* format and was obtained from [Dariel Dato-on on Kaggle](https://www.kaggle.com/oddrationale/mnist-in-csv).\n",
    "\n",
    "The neural network is presented as a Python Class. It has one hidden layer and the number of units is tunable. The neural network is trained by backpropagation. The goal here was not performance but to write everything from scratch, as an exercise.\n",
    "\n",
    "We will use the following modules :\n",
    "* pandas for data import\n",
    "* Numpy for all the calculations, which are mostly vectorized\n",
    "* Matplotlib Pyplot for plotting\n",
    "* time will be used to measure learning time\n",
    "\n",
    "Optionally, the function OneHotEncoder from Scikit-learn can be used to recode the output into a matrix of 0 and 1, but a manual implementation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of a neural network class\n",
    "The `NeuralNet` class contains the functions and parameters which will perform the digit classification. Please read the docstrings and comments for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \"\"\"This class provides method to perform classification of data using\n",
    "a neural network.\n",
    "alpha        : learning rate\n",
    "n_iter       : number of iteration of forward/backward propagation\n",
    "lamb         : lambda parameter for regularization\n",
    "hidden_size  : hidden layer size\n",
    "n_labels     : number of labels, i.e. number of output classes\"\"\"\n",
    "    def __init__(self, alpha=1, n_iter=400, lamb=1, hidden_size=None, n_labels=None):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.lamb = lamb\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "    def rand_weights(self, c_in, c_out, epsilon=0.12):\n",
    "        \"\"\"Return randomly initialized weights of a layer with c_in incoming \n",
    "        connections and c_out and c_out outgoing connection.\"\"\"\n",
    "        return np.random.random((c_out, c_in + 1)) * 2 * epsilon - epsilon\n",
    "\n",
    "    def add_intercept(self, X):\n",
    "        \"\"\"Return matrix with an added column of ones.\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def _s(self, z):\n",
    "        \"\"\"Return product of sigmoid function with z as input.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _sgrad(self, z):\n",
    "        \"\"\"Return the gradient of sigmoid function with z as input.\"\"\"\n",
    "        return self._s(z) * (1 - self._s(z))\n",
    "\n",
    "    def forward_prop(self, X, Y, Theta1, Theta2):\n",
    "        \"\"\"Return values calculated during forward propagation and cost.\"\"\"\n",
    "        z_2 = np.dot(X, Theta1.T)\n",
    "        a_2 = self._s(z_2)\n",
    "        a_2 = self.add_intercept(a_2)\n",
    "        z_3 = np.dot(a_2, Theta2.T)\n",
    "        h = self._s(z_3)\n",
    "\n",
    "        #calculate unregularized cost\n",
    "        m = X.shape[0]\n",
    "        J = np.sum(np.sum(Y * np.log(h) + (1 - Y) * np.log(1 - h))) / -m\n",
    "        #calculate regularization parameters\n",
    "        T1 = Theta1[:, 1:]\n",
    "        T2 = Theta2[:, 1:]\n",
    "        A = self.lamb / 2 / m\n",
    "        B = np.sum(np.sum(T1 * T1, axis=1))\n",
    "        C = np.sum(np.sum(T2 * T2, axis=1))\n",
    "        #calculate regularized cost\n",
    "        J_reg = J + A * (B + C)\n",
    "\n",
    "        return z_2, a_2, z_3, h, J_reg\n",
    "\n",
    "    def backward_prop(self, z_2, a_2, z_3, h, X, Y, Theta1, Theta2, lamb, alpha):\n",
    "        \"\"\"Return updated Theta1 and Theta2.\"\"\"\n",
    "        #perform backward propagation per se\n",
    "        m = Y.shape[0]\n",
    "        T2 = Theta2[:, 1:]\n",
    "        d_3 = h - Y\n",
    "        d_2 = np.dot(d_3, T2) * self._sgrad(z_2)\n",
    "        Delta_1 = np.dot(d_2.T, X)\n",
    "        Delta_2 = np.dot(d_3.T, a_2)\n",
    "\n",
    "        #calculate unregularized gradient\n",
    "        Theta1_grad = Delta_1 / m\n",
    "        Theta2_grad = Delta_2 / m\n",
    "\n",
    "        #regularization of gradient\n",
    "        #replace first column with 0 to avoid regularization of bias\n",
    "        Theta1_copy = np.copy(Theta1)\n",
    "        Theta2_copy = np.copy(Theta2)\n",
    "        Theta1_copy[:, 0] = 0\n",
    "        Theta2_copy[:, 0] = 0\n",
    "        #scale Theta1/2 and add to gradient\n",
    "        Theta1_grad_reg = Theta1_grad + np.dot(lamb / m, Theta1_copy)\n",
    "        Theta2_grad_reg = Theta2_grad + np.dot(lamb / m, Theta2_copy)\n",
    "\n",
    "        #update Theta1/2\n",
    "        Theta1_updated = Theta1 - alpha * Theta1_grad_reg\n",
    "        Theta2_updated = Theta2 - alpha * Theta2_grad_reg\n",
    "\n",
    "        return Theta1_updated, Theta2_updated\n",
    "\n",
    "    def predict(self, Theta1, Theta2, X, Y):\n",
    "        \"\"\"Return predicted output and accuracy of model.\"\"\"\n",
    "        #add intercept to X\n",
    "        X_1 = self.add_intercept(X)\n",
    "\n",
    "        #calculate outputs of neural network layers\n",
    "        h1 = self._s(np.dot(X_1, Theta1.T))\n",
    "        h1_1 = self.add_intercept(h1)\n",
    "        h2 = self._s(np.dot(h1_1, Theta2.T))\n",
    "\n",
    "        #predicted output (remember indexing starts at zero)\n",
    "        #prediction must have the same shape as Y\n",
    "        prediction = (np.argmax(h2, axis=1))[:, np.newaxis]\n",
    "\n",
    "        #accuracy\n",
    "        accuracy = np.mean(prediction == Y)\n",
    "\n",
    "        return prediction, accuracy\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        #add intercept to X\n",
    "        X_1 = self.add_intercept(X)\n",
    "\n",
    "        #get network architecture parameters\n",
    "        in_size = X.shape[1] #input layer size\n",
    "        hid_size = self.hidden_size #hidden layer size\n",
    "        n_lab = self.n_labels #output layer size\n",
    "\n",
    "        #recode Y into one-hot labels\n",
    "        #for Octave data set use Y.item(i) - 1\n",
    "        Y_1 = np.zeros((Y.shape[0], n_lab))\n",
    "        for i in range(Y.shape[0]):\n",
    "            Y_1[i, Y.item(i)] = 1\n",
    "        #could import OneHotEncoder from sklearn.preprocessing\n",
    "        #then Y_1 = OneHotEncoder(sparse=False).fit_transform(Y)\n",
    "        \n",
    "        #initialize Theta matrices randomly\n",
    "        Theta1 = self.rand_weights(in_size, hid_size, 0.12)\n",
    "        Theta2 = self.rand_weights(hid_size, n_lab, 0.12)\n",
    "\n",
    "        #define array to store cost history\n",
    "        cost_history = np.zeros(self.n_iter)\n",
    "\n",
    "        #train the neural network by repetition of for and back prop\n",
    "        for i in range(self.n_iter):\n",
    "            #for prop\n",
    "            z_2, a_2, z_3, h, J_reg = self.forward_prop(X_1, Y_1, Theta1, Theta2)\n",
    "            cost_history[i] = J_reg\n",
    "\n",
    "            #back prop\n",
    "            Theta1, Theta2 = self.backward_prop(z_2, a_2, z_3, h, X_1, Y_1, \n",
    "                                                Theta1, Theta2, \n",
    "                                                self.lamb, self.alpha)\n",
    "\n",
    "        return Theta1, Theta2, cost_history\n",
    "\n",
    "    def plot_history(self, history):\n",
    "        \"Plot cost history to check error of parameters is minimized.\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot([i for i in range(self.n_iter)], history)\n",
    "        ax.set_xlabel(\"Number of iteration\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "        ax.set_title(\"Cost history of neural network parameters\")\n",
    "        plt.show()\n",
    "        \n",
    "    def export_theta(self, filename, Theta1, Theta2):\n",
    "        \"\"\"Export parameters as csv file.\"\"\"\n",
    "        #flatten matrices\n",
    "        thetas = np.concatenate((Theta1.flatten(), Theta2.flatten()))\n",
    "        \n",
    "        #save flat array as csv file\n",
    "        np.savetxt(filename, thetas, delimiter=\",\")\n",
    "        \n",
    "    def import_theta(self, filename, X):\n",
    "        \"\"\"Import csv file containing parameters and convert to matrices of\n",
    "        relevant size. Specify X to reshape Theta1 correctly\"\"\"\n",
    "        #import csv file\n",
    "        data = pd.read_csv(filename, header=None)\n",
    "        weights = np.array(data).flatten()\n",
    "        \n",
    "        #extract and reshape matrices\n",
    "        Theta1 = weights[:self.hidden_size * (X.shape[1] + 1)]\n",
    "        Theta1 = Theta1.reshape((self.hidden_size, X.shape[1] + 1))\n",
    "        Theta2 = weights[self.hidden_size * (X.shape[1] + 1):]\n",
    "        Theta2 = Theta2.reshape((self.n_labels, self.hidden_size + 1))\n",
    "        \n",
    "        return Theta1, Theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network learning and prediction\n",
    "Let's define a model with 60 units in the hidden layer, a learning rate $\\alpha = 0.4$ and a regularization parameter $\\lambda = 1$. The neural network will be trained for 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(hidden_size=60, n_labels=10, n_iter=100, lamb=1, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the data. We have a training set containing 60,000 28 x 28 pixels images, and a testing set containing 10,000 of such images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array(pd.read_csv(\"mnist_train.csv\"))\n",
    "#the first column of data_train contains output values\n",
    "X_train = data_train[:, 1:]\n",
    "Y_train = data_train[:, 0][:, np.newaxis]\n",
    "\n",
    "data_test = np.array(pd.read_csv(\"mnist_test.csv\"))\n",
    "#the first column contains output values\n",
    "X_test = data_test[:, 1:]\n",
    "Y_test = data_test[:, 0][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model on the data. It is possible to select a subset of the whole training set to speed things up. We also measure training time, as you might want to do something else while the neural network is learning. On my computer it takes about one minute with the above parameters, so it's a safe lower boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the neural network : 0 minutes 59 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Theta1, Theta2, cost_history = model.fit(X_train, Y_train)\n",
    "stop = time.time()\n",
    "minutes = \"{} minutes\".format(int((stop - start) // 60))\n",
    "seconds = \"{} seconds\".format(int((stop - start) % 60))\n",
    "print(\"Time to train the neural network :\", minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NeuralNet` class contains functions to determine the accuracy of the model, i.e. the proportion of accurate predictions. We can compare the accuracy on the training set to the accuracy on the testing set, which contains data the model did not process during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training data : 0.9016\n",
      "Accuracy on the testing data : 0.8998\n"
     ]
    }
   ],
   "source": [
    "#accuracy on the training data (pred contains the predicted outputs)\n",
    "pred, accuracy_train = model.predict(Theta1, Theta2, X_train, Y_train)\n",
    "print(\"Accuracy on the training data : {:.4f}\".format(accuracy_train))\n",
    "#accuracy on the testing data\n",
    "pred2, accuracy_test = model.predict(Theta1, Theta2, X_test, Y_test)\n",
    "print(\"Accuracy on the testing data : {:.4f}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
