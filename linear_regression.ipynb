{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with NumPy\n",
    "## Introduction\n",
    "This notebook explains the steps and detail the code to perform a linear regression with NumPy using two methods: gradient descent and the normal equation. Calculation use vectors to maximize speed of execution.\n",
    "I initially wrote this notebook as an exercise to practice these methods and keep a trace of it, I hope it will be useful for somebody. Please comment if you notice mistakes, imprecision, or have a suggestion to make it better.\n",
    "\n",
    "## Gradient descent\n",
    "Gradient descent learns the parameters of a linear model of the data by minimizing the a cost function, which represents the difference between the model and the data. For our purpose, this function is the mean squared error of the linear model. Let's first define the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will need Numpy for calculations, Matplotlib.pyplot for plotting,\n",
    "#and also Pandas for data import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#definition of the cost function\n",
    "def cost(X, Y, theta):\n",
    "    \"\"\"Return the mean squared error based on input matrix X, output vector Y, and linear model\n",
    "    parameters theta\"\"\"\n",
    "    return np.sum((np.dot(X, theta) - Y) ** 2) / 2 / len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the function which performs the gradient descent. In addition to parameters theta of the linear model, it will return the history of the cost so we can visualize how well the gradient descent is doing, and compare between different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, theta, alpha, n_iter):\n",
    "    \"\"\"Return learned parameters theta of linear model as calculated by gradient descent using\n",
    "    input matrix X, output matrix Y, initial parameters theta, learning rate alpha and\n",
    "    number of iteration n_iter\"\"\"\n",
    "    #define matrix to store cost history\n",
    "    cost_history = np.zeros(n_iter)\n",
    "    #loop for n_iter iterations to perform gradient descent\n",
    "    for i in range(n_iter):\n",
    "        delta = np.dot(X.T, (np.dot(X, theta) - Y)) / len(Y) #derivative of cost function\n",
    "        theta -= alpha * delta\n",
    "        cost_history[i] = cost(X, Y, theta)\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cost() and gradient_descent() functions we can now perform a linear regression. Here I use a built-in data set of R, called *state.x77.csv*. I exported it and you can find it in the same repository as this notebook. This data set contains socio-economic data for US states. If you plot pair of variables against each other, you will notice some of them have a pretty good correlation. We will analyze the correlation between the proportion of high school graduate and illiteracy, which have an intuitive relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from the csv file\n",
    "data = pd.read_csv(\"state-x77.csv\")\n",
    "x_origin = data['HS Grad'] #we will need the original data later, I will keep them in this variable\n",
    "y_origin = data['Illiteracy']\n",
    "\n",
    "#plot data\n",
    "_, ax = plt.subplots() #use underscore to define a variable we will not use\n",
    "ax.scatter(x_origin, y_origin)\n",
    "ax.set_xlabel(\"Percentage of high school graduates\")\n",
    "ax.set_ylabel(\"Percentage of illiteracy\")\n",
    "ax.set_title(\"Relationship between high school graduation and illiteracy in US states\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing the gradient descent, we need to transform the data. First we will normalize the data, that is to say we subtract the mean from each data element and divide them by the standard deviation. Normalization is more important when variables in the data set have different orders of magnitude, but I could not get gradient descent work on the data used here without normalization. Then we transpose the data into column-vectors and add a column of \"1\", in order to do vectorized operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature normalization\n",
    "mu = np.mean(x_origin) #calculate the mean\n",
    "sigma = np.std(x_origin) #calculate the standard deviation\n",
    "x = (x_origin - mu) / sigma\n",
    "\n",
    "#x into column-vector and add column of \"1\" for vector operations\n",
    "x = np.array(x)[:, np.newaxis] #x into column-vector\n",
    "x1 = np.ones(len(x), 1) #create column of \"1\"\n",
    "x = np.concatenate((x1, x), axis=1) #add column of \"1\" to x\n",
    "\n",
    "#put y into a column-vector\n",
    "y = np.array(y_origin)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the linear regression by gradient descent. First we set parameters theta to 0 and we will perform 10,000 iterations. It's much more than what is necessary when the learning rate is optimal but a high number of iterations allows us to compare different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((2, 1))\n",
    "n_iter = 10000\n",
    "theta, history = gradient_descent(x, y, theta, alpha=0.01, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we normalized the input before gradient descent, so the parameters theta reflect this. We need to apply a couple calculations to get theta to the same scale as the original data. To understand what operations to do on theta, remember the relationship between theta, x and y : $$y_1 = \\theta_0 + \\theta_1  {x_1 - \\mu_1 \\over \\sigma_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[0] -= theta[1] * mu /sigma\n",
    "theta[1] /= sigma\n",
    "print(\"\\n\\nCalculated theta (linear regression) :\\nintercept :\",\n",
    "      theta[0].flat[0], \"\\nslope :\", theta[1].flat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the data along with the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(x_origin, y_origin, label=\"data\")\n",
    "ax.plot(x_origin, np.dot(x_origin_ones, theta), color=\"C3\", label=\"linear regression\")\n",
    "ax.text(58, 2.4, \"y = {0:.2f} * x + {1:.2f}\".format(theta[1].flat[0], theta[0].flat[0]))\n",
    "ax.set_xlabel(\"Percentage of high school graduates\")\n",
    "ax.set_ylabel(\"Percentage of illiteracy\")\n",
    "ax.set_title(\"Relationship between high school graduation and illiteracy in US states\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the gradient descent with different learning rates to compare the speed at which gradient descent converges towards the minimum cost by plotting the cost VS the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-initialize theta and calculate it with three other learning rate values\n",
    "theta = np.zeros((2, 1))\n",
    "_, history2 = gradient_descent(x, y, theta, alpha=0.003, n_iter=n_iter)\n",
    "theta = np.zeros((2, 1))\n",
    "_, history3 = gradient_descent(x, y, theta, alpha=0.001, n_iter=n_iter)\n",
    "theta = np.zeros((2, 1))\n",
    "_, history4 = gradient_descent(x, y, theta, alpha=0.0003, n_iter=n_iter)\n",
    "\n",
    "#plot history cost calculated during gradient descent\n",
    "x_val = [i for i in range(n_iter)]\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x_val, history, label=\"alpha = 0.01\")\n",
    "ax.plot(x_val, history2, label=\"alpha = 0.003\")\n",
    "ax.plot(x_val, history3, label=\"alpha = 0.001\")\n",
    "ax.plot(x_val, history4, label=\"alpha = 0.0003\")\n",
    "ax.set_xlabel(\"Iteration number\")\n",
    "ax.set_ylabel(\"Cost\")\n",
    "ax.set_title(\"Cost history during gradient descent for different learning rates\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation\n",
    "We can also solve the problem of finding the parameters theta analytically, using the normal equation. This equation calculates theta in a single step, which prevents us from having to choose a learning rate and going through many iterations during the gradients descent. In addition, there is no need to normalize the data. However, if the sample size is large (> 10,000), calculation using the normal equation may be longer than gradient descent. Let's define the function calculating theta using the normal equation and then use it on the same data as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_eq(X, Y):\n",
    "    \"\"\"Return linear regression parameters theta for X and Y by using the normal equation.\"\"\"\n",
    "    return np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "\n",
    "#we need a matrix containing a column of \"1\" and non-normalized data in the other columns\n",
    "x_origin_ones = np.concatenate((x1, x_origin[:, np.newaxis]), axis=1)\n",
    "\n",
    "theta = norm_eq(x_origin_ones, y)\n",
    "\n",
    "print(\"\\n\\nCalculated theta (normal equation) :\\nintercept :\",\n",
    "     theta[0].flat[0], \"\\nslope :\", theta[1].flat[0])\n",
    "\n",
    "#plot data with linear regression line\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(x_origin, y_origin, label=\"data\")\n",
    "ax.plot(x_origin, np.dot(x_origin_ones, theta), color=\"C3\", label=\"linear regression\")\n",
    "ax.text(58, 2.4, \"y = {0:.2f} * x + {1:.2f}\".format(theta[1].flat[0], theta[0].flat[0]))\n",
    "ax.set_xlabel(\"Percentage of high school graduates\")\n",
    "ax.set_ylabel(\"Percentage of illiteracy\")\n",
    "ax.set_title(\"Relationship between high school graduation and illiteracy in US states\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
